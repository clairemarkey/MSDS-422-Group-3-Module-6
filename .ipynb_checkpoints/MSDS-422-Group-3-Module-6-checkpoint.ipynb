{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23cfa2e",
   "metadata": {},
   "source": [
    "## Appendix 1 - Python Code and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67228b03",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c279f9",
   "metadata": {},
   "source": [
    "### Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cba06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# load training data\n",
    "digit_training_data = pd.read_csv('train.csv')\n",
    "\n",
    "# show first rows of the data\n",
    "digit_training_data.head(100)\n",
    "# show number of columns and rows\n",
    "digit_training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4b309",
   "metadata": {},
   "source": [
    "### Investigation of Missing Data and Outliers in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find null counts, percentage of null values, and column type\n",
    "null_count = digit_training_data.isnull().sum()\n",
    "null_percentage = digit_training_data.isnull().sum() * 100 / len(digit_training_data)\n",
    "column_type = digit_training_data.dtypes\n",
    "\n",
    "# show null counts, percentage of null values, and column type for columns with more than one Null value\n",
    "null_summary = pd.concat([null_count, null_percentage, column_type], axis=1, keys=['Missing Count', 'Percentage Missing','Column Type'])\n",
    "null_summary_only_missing = null_summary[null_count != 0].sort_values('Percentage Missing',ascending=False)\n",
    "null_summary_only_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839b249",
   "metadata": {},
   "source": [
    "The above analysis displays that there is no missing data in the digit recognizer training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c80ea",
   "metadata": {},
   "source": [
    "### Import Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test dataset\n",
    "digit_testing_data = pd.read_csv('test.csv')\n",
    "\n",
    "# show first ten rows of the data\n",
    "digit_testing_data.head(10)\n",
    "# show number of columns and rows\n",
    "digit_testing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a1d1b",
   "metadata": {},
   "source": [
    "### Investigation of Missing Data and Outliers in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find null counts, percentage of null values, and column type\n",
    "null_count = digit_testing_data.isnull().sum()\n",
    "null_percentage = digit_testing_data.isnull().sum() * 100 / len(digit_training_data)\n",
    "column_type = digit_testing_data.dtypes\n",
    "\n",
    "# show null counts, percentage of null values, and column type for columns with more than one Null value\n",
    "null_summary = pd.concat([null_count, null_percentage, column_type], axis=1, keys=['Missing Count', 'Percentage Missing','Column Type'])\n",
    "null_summary_only_missing = null_summary[null_count != 0].sort_values('Percentage Missing',ascending=False)\n",
    "null_summary_only_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef3fed",
   "metadata": {},
   "source": [
    "The above analysis displays that there is no missing data in the digit recognizer test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e604a",
   "metadata": {},
   "source": [
    "### Apply Principal Components Analysis (PCA) to Combined Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86695801",
   "metadata": {},
   "source": [
    "First, we will combine the training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the training dataframe\n",
    "pca_train_df = digit_training_data.copy(deep=True)\n",
    "\n",
    "# Drop the label column from the copy of the training dataframe\n",
    "pca_train_df.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the training and test dataframes\n",
    "pca_df = pd.concat([pca_train_df, digit_testing_data])\n",
    "\n",
    "# show first rows of the data\n",
    "pca_df.head(10)\n",
    "# show number of columns and rows\n",
    "pca_df.shape\n",
    "# Describe the dataframe\n",
    "pca_df.describe()\n",
    "\n",
    "\n",
    "# find null counts, percentage of null values, and column type\n",
    "null_count = pca_df.isnull().sum()\n",
    "null_percentage = pca_df.isnull().sum() * 100 / len(digit_training_data)\n",
    "column_type = pca_df.dtypes\n",
    "\n",
    "# show null counts, percentage of null values, and column type for columns with more than one Null value\n",
    "null_summary = pd.concat([null_count, null_percentage, column_type], axis=1, keys=['Missing Count', 'Percentage Missing','Column Type'])\n",
    "null_summary_only_missing = null_summary[null_count != 0].sort_values('Percentage Missing',ascending=False)\n",
    "null_summary_only_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9851d7",
   "metadata": {},
   "source": [
    "Next, we scale the data to prepare it for our principal components analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale PCA dataframe's data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "pca_scaled = sc.fit_transform(pca_df) # normalizing the features\n",
    "\n",
    "# Convert scaled data from numpy array into dataframe\n",
    "pca_features = list(pca_df.columns.values)\n",
    "pca_scaled_df = pd.DataFrame(pca_scaled, columns=pca_features)\n",
    "\n",
    "# Confirm scaling transformation was a success\n",
    "pca_scaled_df.shape\n",
    "pca_scaled_df.head(10)\n",
    "pca_scaled_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff205366",
   "metadata": {},
   "source": [
    "We also apply this scaling to our test dataframe for later use as we progress through the construction of our Principal Component Analysis and Random Forest model creation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the standard scaling to the test dataframe\n",
    "pca_test_scaled = sc.transform(digit_testing_data)\n",
    "\n",
    "# Convert scaled data from numpy array into dataframe\n",
    "pca_test_features = list(digit_testing_data.columns.values)\n",
    "pca_test_scaled_df = pd.DataFrame(pca_test_scaled, columns=pca_test_features)\n",
    "\n",
    "# Confirm scaling transformation was a success\n",
    "pca_test_scaled_df.shape\n",
    "pca_test_scaled_df.head(10)\n",
    "pca_test_scaled_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefd392",
   "metadata": {},
   "source": [
    "Next, we will conduct a Principal Components Analysis to identify principal components that account for at least 95% of the variation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b583a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the Principal Components Analysis\n",
    "import datetime\n",
    "pca_start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Applying PCA function on training and testing set of X component\n",
    "from sklearn.decomposition import PCA\n",
    "pca_digits_train_test = PCA(n_components=334)\n",
    "principal_components_digits = pca_digits_train_test.fit_transform(pca_scaled_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a Cumulative Scree plot to help us determine how many principal components to include in our random forest model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "PC_values = np.arange(pca_digits_train_test.n_components_) + 1\n",
    "cumulative_explained_variance_pca = np.cumsum(pca_digits_train_test.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(PC_values, cumulative_explained_variance_pca, 'o-', linewidth=1, color='blue')\n",
    "plt.title('Cumulative Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe to display the information in the cumulative scree plot in a different manner\n",
    "scree_df = pd.DataFrame({'Principal Component':PC_values, 'Variance Explained':cumulative_explained_variance_pca})\n",
    "scree_df\n",
    "\n",
    "# Create a dataframe that contains the principal component values for each of the observations in the pca dataframe\n",
    "pca_column_list = []\n",
    "for num in range(1, 335):\n",
    "    pca_column_list.append(\"PC_\" + str(num))\n",
    "\n",
    "pca_digits_df = pd.DataFrame(data = principal_components_digits , columns = pca_column_list )\n",
    "\n",
    "pca_digits_df\n",
    "\n",
    "\n",
    "# Print the run time for Python to complete the Principal Components Analysis\n",
    "pca_end = datetime.datetime.now()\n",
    "pca_runtime = pca_end - pca_start\n",
    "print(f\"The total run time for the Principal Components Analysis was {pca_runtime}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a34668",
   "metadata": {},
   "source": [
    "### Construct a Random Forest Model Using the Principal Components Identified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fac025",
   "metadata": {},
   "source": [
    "Let's fit a Random Forest Model to predict digits using the principal components just identified.  We will use our training and validation datasets to conduct hyperparameter tuning to find the best hyperparameters for random forest modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f199a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the Random Forest\n",
    "\n",
    "pca_rf_start = datetime.datetime.now()\n",
    "\n",
    "# Create the Random Forest Model\n",
    "\n",
    "# Import Required Modules\n",
    "#pip install graphviz\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# Split the training dataset into predictor and outcome components\n",
    "rf_train_validation_x = pca_digits_df.copy(deep=True)\n",
    "rf_train_validation_x.drop(rf_train_validation_x.tail(28000).index, inplace = True)\n",
    "rf_train_validation_y = digit_training_data['label']\n",
    "\n",
    "# Split the Kaggle training data into training and validation components\n",
    "rf_x_train, rf_x_validation, rf_y_train, rf_y_validation = train_test_split(rf_train_validation_x,\n",
    "                                                                      rf_train_validation_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                           random_state = 1)\n",
    "\n",
    "# Conduct hyperparameter tuning for random forest models\n",
    "param_dist = {'n_estimators': randint(10,100),\n",
    "              'max_depth': randint(1,100),\n",
    "             'max_features': randint(1,20)}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "rand_search.fit(rf_x_train, rf_y_train)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "# Print the run time for Python to complete the Random Forest\n",
    "pca_rf_end = datetime.datetime.now()\n",
    "pca_rf_runtime = pca_rf_end - pca_rf_start\n",
    "print(f\"The total run time for the Random Forest Model using the principal components was {pca_rf_runtime}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc5c18",
   "metadata": {},
   "source": [
    "Next, we will assess the strength of the random forest model associated with the optimal hyperparameters by applying the model to the validation dataset and observing the resulting confusion matrix and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate predictions with the best model\n",
    "y_validation_predictions_rf = best_rf.predict(rf_x_validation)\n",
    "\n",
    "# Create the confusion matrix associated with the best random forest model\n",
    "cm = confusion_matrix(rf_y_validation, y_validation_predictions_rf)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot();\n",
    "\n",
    "# Calculate the accuracy, precision, and recall associated with the predictions of the best random forest model\n",
    "\n",
    "accuracy_rf_validation = accuracy_score(rf_y_validation, y_validation_predictions_rf)\n",
    "#precision_rf_validation = precision_score(rf_y_validation, y_validation_predictions_rf)\n",
    "#recall_rf_validation = recall_score(rf_y_validation, y_validation_predictions_rf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_rf_validation)\n",
    "#print(\"Precision:\", precision_rf_validation)\n",
    "#print(\"Recall:\", recall_rf_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b31785",
   "metadata": {},
   "source": [
    "Apply the Random Forest Model to the Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for predictor variables in the test dataframe for random forest model\n",
    "#rf_testing_x = rf_testing_df.drop(columns=['PassengerId'])\n",
    "rf_testing_x = pca_digits_df.copy(deep=True)\n",
    "rf_testing_x.drop(rf_testing_x.head(42000).index, inplace = True)\n",
    "\n",
    "# Apply the Random Forest model to the test dataset\n",
    "y_test_predictions_rf = best_rf.predict(rf_testing_x)\n",
    "\n",
    "# Put the random forest predictions into a Pandas dataframe\n",
    "prediction_df_rf = pd.DataFrame(y_test_predictions_rf, columns=['Label'])\n",
    "\n",
    "# Add the ID column to the front of the random forest predictions dataframe\n",
    "ImageId_series = pd.Series(range(1,28001))\n",
    "prediction_df_rf.insert(0, 'ImageId', ImageId_series)\n",
    "\n",
    "#output predictions to csv\n",
    "prediction_df_rf.to_csv('test_predictions_pca_random_forest_v1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12abf7",
   "metadata": {},
   "source": [
    "Let's display the Kaggle results from the application of the random forest model using principal components to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87824f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the kaggle results associated with the Random Forest Model\n",
    "plt.figure(figsize = (15, 15))\n",
    "kaggle_results = plt.imread('Digit_PCA_Random_Forest_Kaggle_Results_v1.jpg')\n",
    "plt.imshow(kaggle_results)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c2716",
   "metadata": {},
   "source": [
    "### Construct a Random Forest Model Using the Principal Components Identified and the Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51734160",
   "metadata": {},
   "source": [
    "Let's fit a Random Forest Model to predict digits using the principal components and the original underlying data.  We will use our training and validation datasets to conduct hyperparameter tuning to find the best hyperparameters for random forest modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a424593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the Random Forest\n",
    "\n",
    "pca_rf_v2_start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "# Split the training dataset into predictor and outcome components\n",
    "rf_train_validation_x = pca_digits_df.copy(deep=True)\n",
    "rf_train_validation_x.drop(rf_train_validation_x.tail(28000).index, inplace = True)\n",
    "rf_train_validation_x = pd.concat([rf_train_validation_x, pca_train_df], axis=1)\n",
    "rf_train_validation_y = digit_training_data['label']\n",
    "\n",
    "# Split the Kaggle training data into training and validation components\n",
    "rf_x_train, rf_x_validation, rf_y_train, rf_y_validation = train_test_split(rf_train_validation_x,\n",
    "                                                                      rf_train_validation_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                           random_state = 1)\n",
    "\n",
    "# Conduct hyperparameter tuning for random forest models\n",
    "param_dist = {'n_estimators': randint(10,100),\n",
    "              'max_depth': randint(1,100),\n",
    "             'max_features': randint(1,20)}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "rand_search.fit(rf_x_train, rf_y_train)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "\n",
    "# Print the run time for Python to complete the Random Forest\n",
    "pca_rf_v2_end = datetime.datetime.now()\n",
    "pca_rf_v2_runtime = pca_rf_v2_end - pca_rf_v2_start\n",
    "print(f\"The total run time for the Random Forest Model using the principal components and original pixel features was {pca_rf_v2_runtime}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428225f",
   "metadata": {},
   "source": [
    "Next, we will assess the strength of the random forest model associated with the optimal hyperparameters by applying the model to the validation dataset and observing the resulting confusion matrix and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fe91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with the best model\n",
    "y_validation_predictions_rf = best_rf.predict(rf_x_validation)\n",
    "\n",
    "# Create the confusion matrix associated with the best random forest model\n",
    "cm = confusion_matrix(rf_y_validation, y_validation_predictions_rf)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot();\n",
    "\n",
    "# Calculate the accuracy, precision, and recall associated with the predictions of the best random forest model\n",
    "\n",
    "accuracy_rf_validation = accuracy_score(rf_y_validation, y_validation_predictions_rf)\n",
    "#precision_rf_validation = precision_score(rf_y_validation, y_validation_predictions_rf)\n",
    "#recall_rf_validation = recall_score(rf_y_validation, y_validation_predictions_rf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_rf_validation)\n",
    "#print(\"Precision:\", precision_rf_validation)\n",
    "#print(\"Recall:\", recall_rf_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b36b2",
   "metadata": {},
   "source": [
    "Apply the Random Forest Model to the Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for predictor variables in the test dataframe for random forest model\n",
    "rf_testing_x = pca_digits_df.copy(deep=True)\n",
    "rf_testing_x.drop(rf_testing_x.head(42000).index, inplace = True)\n",
    "rf_testing_x.reset_index(drop=True, inplace=True)\n",
    "digit_testing_data.reset_index(drop=True, inplace=True)\n",
    "rf_testing_x = pd.concat([rf_testing_x, digit_testing_data], axis=1)\n",
    "\n",
    "# Apply the Random Forest model to the test dataset\n",
    "y_test_predictions_rf = best_rf.predict(rf_testing_x)\n",
    "\n",
    "# Put the random forest predictions into a Pandas dataframe\n",
    "prediction_df_rf = pd.DataFrame(y_test_predictions_rf, columns=['Label'])\n",
    "\n",
    "# Add the ID column to the front of the random forest predictions dataframe\n",
    "ImageId_series = pd.Series(range(1,28001))\n",
    "prediction_df_rf.insert(0, 'ImageId', ImageId_series)\n",
    "\n",
    "#output predictions to csv\n",
    "prediction_df_rf.to_csv('test_predictions_pca_random_forest_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbf17e",
   "metadata": {},
   "source": [
    "Let's display the Kaggle results from the application of the random forest model using principal components and the original underlying data features to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2092872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the kaggle results associated with the Random Forest Model\n",
    "plt.figure(figsize = (15, 15))\n",
    "kaggle_results = plt.imread('Digit_PCA_And_Original_Features_Random_Forest_Kaggle_Results_v1.jpg')\n",
    "plt.imshow(kaggle_results)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f4c3c",
   "metadata": {},
   "source": [
    "### Deploy K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0f5a9",
   "metadata": {},
   "source": [
    "Let's use K-means clustering to predict digits using original features. First let's create our training and testing data and plot the digits in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Split the training dataset into predictor and outcome variables\n",
    "kmeans_x = digit_training_data.copy(deep=True)\n",
    "kmeans_x.drop(['label'], axis=1, inplace=True)\n",
    "kmeans_y = digit_training_data['label']\n",
    "\n",
    "\n",
    "kmeans_x_train = np.array(kmeans_x_train)\n",
    "kmeans_y_train = np.array(kmeans_y_train)\n",
    "\n",
    "\n",
    "print('Training Data: {}'.format(kmeans_x_train.shape))\n",
    "print('Training Labels: {}'.format(kmeans_y_train.shape))\n",
    "\n",
    "# reshape array to 3-dimensional array so we can plot the numbers\n",
    "kmeans_x_train_plot = kmeans_x_train.reshape(33600, 28, 28)\n",
    "\n",
    "# Plot the digits in the dataset\n",
    "fig, axs = plt.subplots(3, 3, figsize = (12, 12))\n",
    "plt.gray()\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.matshow(kmeans_x_train_plot[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Number {}'.format(kmeans_y_train[i]))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef127dc",
   "metadata": {},
   "source": [
    "Normalize the training data before applying k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275078e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "kmeans_x_train_norm = preprocessing.normalize(kmeans_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7dabd",
   "metadata": {},
   "source": [
    "The MNIST dataset contains images of the integers 0 to 9. Because of this, let’s start by setting the number of clusters to 10, one for each digit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a5358",
   "metadata": {},
   "source": [
    "Compute the silhouette coefficients kmeans models with different numbers of clusters. This can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.\n",
    "\n",
    "reference: Geron, Aurelien. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 2nd ed. Sebastopol, CA: O'Reilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "# minibatchkmeans has a memeory leak warning that we can ignore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# create k-means models with 1 - 12 clusters. \n",
    "K = range(20,22)\n",
    "\n",
    "# Store within-cluster-sum of squares and silhouette scores for clusters\n",
    "wss = []\n",
    "sil_score = []\n",
    "\n",
    "# loop though cluster values and save inertia and silhouttee values\n",
    "for i in K:\n",
    "    kmeans=MiniBatchKMeans(n_clusters=i, random_state=1)\n",
    "    kmeans=kmeans.fit(kmeans_x_train_norm)\n",
    "    # within-cluster-sum-squares\n",
    "    wss_iter = kmeans.inertia_\n",
    "    wss.append(wss_iter)\n",
    "    # silhouttee score\n",
    "    score = silhouette_score(kmeans_x_train_norm, kmeans.labels_)\n",
    "    sil_score.append(score)\n",
    "    print (\"Silhouette score for k(clusters) = \"+str(i)+\" is \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# elbow and silhouttee scores in dataframe with number of clusters\n",
    "cluster_sil_scores = pd.DataFrame({'Clusters' : K, 'WSS' : wss, 'Sil Score' : sil_score})\n",
    "cluster_sil_scores\n",
    "\n",
    "# plot the elbow scores\n",
    "sns.lineplot(x = 'Clusters', y = 'WSS', data = cluster_sil_scores, marker=\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95707086",
   "metadata": {},
   "source": [
    "Based on the elbow plot, the inertia drops very quickly as we increase k up to 9, but then it decreases a bit more slowly as we keep increasing k. This curve doesn't have much of an elbow shape, but based on what we see, 8 clusters appears to be ideal for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the silhouttee scores\n",
    "sns.lineplot(x = 'Clusters', y = 'Sil Score', data = cluster_sil_scores, marker=\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267d8a1",
   "metadata": {},
   "source": [
    "Based on the plot of silhouette scores, the highest silhouettee coefficient is for 8 clusters. That alligns with what we observed on the elbow plot with the inertia values.\n",
    "\n",
    "We will try building a k-means model with 8 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc0fa0",
   "metadata": {},
   "source": [
    "K-means clustering is an unsupervised machine learning method so the labels assigned by our KMeans algorithm refer to the cluster each array was assigned to, not the actual target integer. This section defines functions that predict which integer corresponds to each cluster. reference: https://medium.datadriveninvestor.com/k-means-clustering-for-imagery-analysis-56c9976f16b6#:~:text=Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_cluster_labels(kmeans, actual_labels):\n",
    "    inferred_labels = {}\n",
    "\n",
    "    for i in range(kmeans.n_clusters):\n",
    "\n",
    "        # find index of points in cluster\n",
    "        labels = []\n",
    "        index = np.where(kmeans.labels_ == i)\n",
    "\n",
    "        # append actual labels for each point in cluster\n",
    "        labels.append(actual_labels[index])\n",
    "\n",
    "        # determine most common label\n",
    "        if len(labels[0]) == 1:\n",
    "            counts = np.bincount(labels[0])\n",
    "        else:\n",
    "            counts = np.bincount(np.squeeze(labels))\n",
    "\n",
    "        # assign the cluster to a value in the inferred_labels dictionary\n",
    "        if np.argmax(counts) in inferred_labels:\n",
    "            # append the new number to the existing array at this slot\n",
    "            inferred_labels[np.argmax(counts)].append(i)\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "            inferred_labels[np.argmax(counts)] = [i]\n",
    "\n",
    "        #print(labels)\n",
    "        #print('Cluster: {}, label: {}'.format(i, np.argmax(counts)))\n",
    "\n",
    "    return inferred_labels\n",
    "\n",
    "def infer_data_labels(X_labels, cluster_labels):\n",
    "  # empty array of len(X)\n",
    "    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)\n",
    "\n",
    "    for i, cluster in enumerate(X_labels):\n",
    "        for key, value in cluster_labels.items():\n",
    "            if cluster in value:\n",
    "                predicted_labels[i] = key\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "# test the infer_cluster_labels() and infer_data_labels() functions\n",
    "\n",
    "cluster_labels = infer_cluster_labels(kmeans, kmeans_y_train)\n",
    "\n",
    "# create figure with subplots using matplotlib.pyplot\n",
    "fig, axs = plt.subplots(6, 6, figsize = (20, 20))\n",
    "plt.gray()\n",
    "\n",
    "# loop through subplots and add centroid images\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    \n",
    "    # determine inferred label using cluster_labels dictionary\n",
    "    for key, value in cluster_labels.items():\n",
    "        if i in value:\n",
    "            ax.set_title('Inferred Label: {}'.format(key))\n",
    "    \n",
    "    # add image to subplot\n",
    "    ax.matshow(images[i])\n",
    "    ax.axis('off')\n",
    "    \n",
    "# display the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953278cd",
   "metadata": {},
   "source": [
    "Let's build a model with 8 clusters, since this is the optimal cluster number base don our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3583b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Initialize KMeans model\n",
    "kmeans = MiniBatchKMeans(n_clusters = 36, random_state=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "kmeans.fit(kmeans_x_train_norm)\n",
    "\n",
    "# Predict the cluster assignment\n",
    "X_clusters = kmeans.predict(kmeans_x_train_norm)\n",
    "print(X_clusters[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "\n",
    "# predict labels on training data\n",
    "\n",
    "cluster_labels=infer_cluster_labels(kmeans,kmeans_y_train)\n",
    "X_clusters = kmeans.predict(kmeans_x_train_norm)\n",
    "predicted_labels = infer_data_labels(X_clusters, cluster_labels)\n",
    "print(predicted_labels[:20])\n",
    "print(kmeans_y_train[:20])\n",
    "\n",
    "# Create the confusion matrix associated with the best random forest model\n",
    "cm = confusion_matrix(kmeans_y_train, predicted_labels)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot();\n",
    "\n",
    "# Calculate the accuracy, precision, and recall associated with the predictions of the best random forest model\n",
    "\n",
    "accuracy_kmeans = accuracy_score(kmeans_y_train, predicted_labels)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record centroid values\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# reshape centroids into images\n",
    "images = centroids.reshape(36, 28, 28)\n",
    "images *= 255\n",
    "images = images.astype(np.uint8)\n",
    "\n",
    "# determine cluster labels\n",
    "cluster_labels = infer_cluster_labels(kmeans, kmeans_y_train)\n",
    "\n",
    "# create figure with subplots using matplotlib.pyplot\n",
    "fig, axs = plt.subplots(6, 6, figsize = (20, 20))\n",
    "plt.gray()\n",
    "\n",
    "# loop through subplots and add centroid images\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    \n",
    "    # determine inferred label using cluster_labels dictionary\n",
    "    for key, value in cluster_labels.items():\n",
    "        if i in value:\n",
    "            ax.set_title('Inferred Label: {}'.format(key))\n",
    "    \n",
    "    # add image to subplot\n",
    "    ax.matshow(images[i])\n",
    "    ax.axis('off')\n",
    "    \n",
    "# display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf0b3a",
   "metadata": {},
   "source": [
    "Apply the K-means Clustering Model to the Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for predictor variables in the test dataframe for kmeans model\n",
    "kmeans_testing_x = digit_training_data.copy(deep=True)\n",
    "kmeans_testing_x.drop(kmeans_testing_x.head(42000).index, inplace = True)\n",
    "\n",
    "# Apply the kmeans model to the test dataset\n",
    "y_test_predictions_kmeans = kmeans.predict(kmeans_testing_x)\n",
    "\n",
    "# Put the kmeans predictions into a Pandas dataframe\n",
    "prediction_df_kmeans = pd.DataFrame(y_test_predictions_kmeans, columns=['Label'])\n",
    "\n",
    "# Add the ID column to the front of the kmeans predictions dataframe\n",
    "ImageId_series = pd.Series(range(1,28001))\n",
    "prediction_df_kmeans.insert(0, 'ImageId', ImageId_series)\n",
    "\n",
    "# Output predictions to csv\n",
    "prediction_df_kmeans.to_csv('test_predictions_kmeans_v1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ceaca9",
   "metadata": {},
   "source": [
    "Let's display the Kaggle results from the application of the kmeans model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the kaggle results associated with the Random Forest Model\n",
    "plt.figure(figsize = (15, 15))\n",
    "kaggle_results = plt.imread('Digit_Kmeans_v1.jpg')\n",
    "plt.imshow(kaggle_results)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
